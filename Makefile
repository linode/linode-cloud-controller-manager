IMG                     ?= linode/linode-cloud-controller-manager:canary
RELEASE_DIR             ?= release
PLATFORM                ?= linux/amd64

# Use CACHE_BIN for tools that cannot use devbox and LOCALBIN for tools that can use either method
CACHE_BIN               ?= $(CURDIR)/bin
LOCALBIN                ?= $(CACHE_BIN)

DEVBOX_BIN              ?= $(DEVBOX_PACKAGES_DIR)/bin

#####################################################################
# Dev Setup
#####################################################################
CLUSTER_NAME            ?= ccm-$(shell git rev-parse --short HEAD)
SUBNET_CLUSTER_NAME     ?= subnet-testing-$(shell git rev-parse --short HEAD)
VPC_NAME                ?= $(CLUSTER_NAME)
MANIFEST_NAME           ?= capl-cluster-manifests
SUBNET_MANIFEST_NAME    ?= subnet-testing-manifests
CONTROLPLANE_NODES      ?= 1
WORKER_NODES            ?= 1
LINODE_FIREWALL_ENABLED ?= true
LINODE_REGION           ?= us-lax
LINODE_OS               ?= linode/ubuntu22.04
LINODE_URL              ?= https://api.linode.com
KUBECONFIG_PATH         ?= $(CURDIR)/test-cluster-kubeconfig.yaml
SUBNET_KUBECONFIG_PATH	?= $(CURDIR)/subnet-testing-kubeconfig.yaml
MGMT_KUBECONFIG_PATH    ?= $(CURDIR)/mgmt-cluster-kubeconfig.yaml

# if the $DEVBOX_PACKAGES_DIR env variable exists that means we are within a devbox shell and can safely
# use devbox's bin for our tools
ifdef DEVBOX_PACKAGES_DIR
	LOCALBIN = $(DEVBOX_BIN)
endif

export PATH := $(CACHE_BIN):$(PATH)
$(LOCALBIN):
	mkdir -p $(LOCALBIN)

export GO111MODULE=on

.PHONY: all
all: build

## --------------------------------------
## Cleanup
## --------------------------------------

##@ Cleanup:

.PHONY: clean
clean:
	@go clean .
	@rm -rf ./.tmp
	@rm -rf dist/*
	@rm -rf $(RELEASE_DIR)
	@rm -rf $(LOCALBIN)

## --------------------------------------
## Development
## --------------------------------------

##@ Development:

.PHONY: fmt
fmt: ## Run go fmt against code.
	go fmt ./...

.PHONY: vet
vet: ## Run go vet against code.
	go vet ./...

.PHONY: gosec
gosec: tools ## Run gosec against code.
	gosec -exclude-dir=bin -exclude-generated ./...

.PHONY: lint
lint: tools ## Run lint against code.
	golangci-lint run -c .golangci.yml

.PHONY: lint-fix
lint-fix: tools ## Run lint against code.
	golangci-lint run -c .golangci.yml --fix

.PHONY: nilcheck
nilcheck: tools ## Run nil check against code.
	go list ./... | xargs -I {} -d '\n' nilaway -include-pkgs {} -exclude-file-docstrings "ignore_autogenerated" ./...

.PHONY: vulncheck
vulncheck: tools ## Run vulnerability check against code.
	govulncheck ./...

.PHONY: codegen
codegen:
	go generate ./...

## --------------------------------------
## Testing
## --------------------------------------

##@ Testing:

.PHONY: test
# we say code is not worth testing unless it's formatted
test: fmt codegen vet
	go test -v -coverpkg=./sentry,./cloud/linode/client,./cloud/linode,./cloud/linode/utils,./cloud/linode/services,./cloud/nodeipam,./cloud/nodeipam/ipam -coverprofile ./coverage.out -cover ./sentry/... ./cloud/... $(TEST_ARGS)

## --------------------------------------
## Build
## --------------------------------------

.PHONY: build-linux
build-linux: codegen fmt vet
	echo "cross compiling linode-cloud-controller-manager for linux/amd64" && \
		GOOS=linux GOARCH=amd64 \
		CGO_ENABLED=0 \
		go build -o dist/linode-cloud-controller-manager-linux-amd64 .

.PHONY: build
build: codegen fmt vet
	echo "compiling linode-cloud-controller-manager" && \
		CGO_ENABLED=0 \
		go build -o dist/linode-cloud-controller-manager .

.PHONY: release
release:
	mkdir -p $(RELEASE_DIR)
	sed -e 's/appVersion: "latest"/appVersion: "$(IMAGE_VERSION)"/g' ./deploy/chart/Chart.yaml
	tar -czvf ./$(RELEASE_DIR)/helm-chart-$(IMAGE_VERSION).tgz -C ./deploy/chart .

.PHONY: imgname
# print the Docker image name that will be used
# useful for subsequently defining it on the shell
imgname:
	echo IMG=${IMG}

.PHONY: docker-build
# we cross compile the binary for linux, then build a container
docker-build: build-linux
	DOCKER_BUILDKIT=1 docker build --platform=$(PLATFORM) --tag ${IMG} .

.PHONY: docker-push
# must run the docker build before pushing the image
docker-push:
	docker push ${IMG}

.PHONY: docker-setup
docker-setup: docker-build docker-push

.PHONY: run
# run the ccm locally, really only makes sense on linux anyway
run: build
	dist/linode-cloud-controller-manager \
		--logtostderr=true \
		--stderrthreshold=INFO \
		--kubeconfig=${KUBECONFIG}

.PHONY: run-debug
# run the ccm locally, really only makes sense on linux anyway
run-debug: build
	dist/linode-cloud-controller-manager \
		--logtostderr=true \
		--stderrthreshold=INFO \
		--kubeconfig=${KUBECONFIG} \
		--linodego-debug

#####################################################################
# E2E Test Setup
#####################################################################

.PHONY: mgmt-and-capl-cluster
mgmt-and-capl-cluster: tools docker-setup mgmt-cluster capl-cluster

.PHONY: capl-cluster
capl-cluster: tools generate-capl-cluster-manifests create-capl-cluster patch-linode-ccm

.PHONY: generate-capl-cluster-manifests
generate-capl-cluster-manifests: tools
	# Create the CAPL cluster manifests without any CSI driver stuff
	LINODE_FIREWALL_ENABLED=$(LINODE_FIREWALL_ENABLED) LINODE_OS=$(LINODE_OS) VPC_NAME=$(VPC_NAME) clusterctl generate cluster $(CLUSTER_NAME) \
		--infrastructure linode-linode \
		--control-plane-machine-count $(CONTROLPLANE_NODES) --worker-machine-count $(WORKER_NODES) > $(MANIFEST_NAME).yaml
	yq -i e 'select(.kind == "LinodeVPC").spec.subnets = [{"ipv4": "10.0.0.0/8", "label": "default"}, {"ipv4": "172.16.0.0/16", "label": "testing"}]' $(MANIFEST_NAME).yaml

.PHONY: create-capl-cluster
create-capl-cluster: tools
	# Create a CAPL cluster with updated CCM and wait for it to be ready
	kubectl apply -f $(MANIFEST_NAME).yaml
	kubectl wait --for=condition=ControlPlaneReady cluster/$(CLUSTER_NAME) --timeout=600s || (kubectl get cluster -o yaml; kubectl get linodecluster -o yaml; kubectl get linodemachines -o yaml)
	kubectl wait --for=condition=NodeHealthy=true machines -l cluster.x-k8s.io/cluster-name=$(CLUSTER_NAME) --timeout=900s
	clusterctl get kubeconfig $(CLUSTER_NAME) > $(KUBECONFIG_PATH)
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl wait --for=condition=Ready nodes --all --timeout=600s
	# Remove all taints from control plane node so that pods scheduled on it by tests can run (without this, some tests fail)
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl taint nodes -l node-role.kubernetes.io/control-plane node-role.kubernetes.io/control-plane-

.PHONY: patch-linode-ccm
patch-linode-ccm: tools
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl patch -n kube-system daemonset ccm-linode --type='json' -p="[{'op': 'replace', 'path': '/spec/template/spec/containers/0/image', 'value': '${IMG}'}]"
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl patch -n kube-system daemonset ccm-linode --type='json' -p='[{"op": "add", "path": "/spec/template/spec/containers/0/env/-", "value": {"name": "LINODE_API_VERSION", "value": "v4beta"}}]'
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl rollout status -n kube-system daemonset/ccm-linode --timeout=600s
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl -n kube-system get daemonset/ccm-linode -o yaml

.PHONY: mgmt-cluster
mgmt-cluster: tools
	# Create a mgmt cluster
	ctlptl apply -f e2e/setup/ctlptl-config.yaml
	clusterctl init \
		--wait-providers \
		--wait-provider-timeout 600 \
		--addon helm \
		--infrastructure linode-linode
	kind get kubeconfig --name=caplccm > $(MGMT_KUBECONFIG_PATH)

.PHONY: cleanup-cluster
cleanup-cluster: tools
	kubectl delete cluster -A --all --timeout=180s
	kubectl delete linodefirewalls -A --all --timeout=60s
	kubectl delete lvpc -A --all --timeout=60s
	kind delete cluster -n caplccm

.PHONY: e2e-test
e2e-test: tools
	CLUSTER_NAME=$(CLUSTER_NAME) \
	MGMT_KUBECONFIG=$(MGMT_KUBECONFIG_PATH) \
	KUBECONFIG=$(KUBECONFIG_PATH) \
	REGION=$(LINODE_REGION) \
	LINODE_TOKEN=$(LINODE_TOKEN) \
	LINODE_URL=$(LINODE_URL) \
	chainsaw test e2e/test --parallel 2 $(E2E_FLAGS)

.PHONY: e2e-test-bgp
e2e-test-bgp: tools
	KUBECONFIG=$(KUBECONFIG_PATH) CLUSTER_SUFFIX=$(CLUSTER_NAME) ./e2e/setup/cilium-setup.sh
	KUBECONFIG=$(KUBECONFIG_PATH) kubectl -n kube-system rollout status daemonset/ccm-linode --timeout=300s
	CLUSTER_NAME=$(CLUSTER_NAME) \
		MGMT_KUBECONFIG=$(MGMT_KUBECONFIG_PATH) \
		KUBECONFIG=$(KUBECONFIG_PATH) \
		REGION=$(LINODE_REGION) \
		LINODE_TOKEN=$(LINODE_TOKEN) \
		LINODE_URL=$(LINODE_URL) \
		chainsaw test e2e/bgp-test/lb-cilium-bgp $(E2E_FLAGS)

.PHONY: e2e-test-subnet
e2e-test-subnet: tools
	# Generate cluster manifests for second cluster
	SUBNET_NAME=testing CLUSTER_NAME=$(SUBNET_CLUSTER_NAME) MANIFEST_NAME=$(SUBNET_MANIFEST_NAME) VPC_NAME=$(CLUSTER_NAME) \
		VPC_NETWORK_CIDR=172.16.0.0/16 K8S_CLUSTER_CIDR=172.16.64.0/18 make generate-capl-cluster-manifests
	# Add subnetNames to HelmChartProxy	
	yq e 'select(.kind == "HelmChartProxy" and .spec.chartName == "ccm-linode").spec.valuesTemplate' $(SUBNET_MANIFEST_NAME).yaml > tmp.yaml
	yq -i e '.routeController  += {"subnetNames": "testing"}' tmp.yaml
	yq -i e '.routeController.vpcNames = "{{.InfraCluster.spec.vpcRef.name}}"' tmp.yaml
	yq -i e 'select(.kind == "HelmChartProxy" and .spec.chartName == "ccm-linode").spec.valuesTemplate = load_str("tmp.yaml")' $(SUBNET_MANIFEST_NAME).yaml
	rm tmp.yaml
	# Create the second cluster
	MANIFEST_NAME=$(SUBNET_MANIFEST_NAME) CLUSTER_NAME=$(SUBNET_CLUSTER_NAME) KUBECONFIG_PATH=$(SUBNET_KUBECONFIG_PATH) \
		make create-capl-cluster
	KUBECONFIG_PATH=$(SUBNET_KUBECONFIG_PATH) make patch-linode-ccm
	# Run chainsaw test
	LINODE_TOKEN=$(LINODE_TOKEN) \
		LINODE_URL=$(LINODE_URL) \
		FIRST_CONFIG=$(KUBECONFIG_PATH) \
		SECOND_CONFIG=$(SUBNET_KUBECONFIG_PATH) \
		chainsaw test e2e/subnet-test $(E2E_FLAGS)

#####################################################################
# OS / ARCH
#####################################################################

# Set the host's OS. Only linux and darwin supported for now
HOSTOS := $(shell uname -s | tr '[:upper:]' '[:lower:]')
ifeq ($(filter darwin linux,$(HOSTOS)),)
$(error build only supported on linux and darwin host currently)
endif
ARCH=$(shell uname -m)
ARCH_SHORT=$(ARCH)
ifeq ($(ARCH_SHORT),x86_64)
ARCH_SHORT := amd64
else ifeq ($(ARCH_SHORT),aarch64)
ARCH_SHORT := arm64
endif

## --------------------------------------
## Tooling Binaries
## --------------------------------------

##@ Tooling Binaries:
HELM                    ?= $(LOCALBIN)/helm

## Tool Versions
CLUSTERCTL_VERSION       ?= v1.11.1
KUBECTL_VERSION          ?= v1.28.0
CHAINSAW_VERSION         ?= v0.2.13
HELM_VERSION             ?= v3.16.3

.PHONY: tools
tools: $(CLUSTERCTL) $(KUBECTL)
	go install tool
##@ we can't manage this with go tools because it causes a panic due to missing CRDs when running chainsaw
	go install github.com/kyverno/chainsaw@$(CHAINSAW_VERSION)

.PHONY: helm
helm: $(HELM) ## Download helm locally if necessary
$(HELM): $(LOCALBIN)
	@curl -fsSL https://get.helm.sh/helm-$(HELM_VERSION)-$(HOSTOS)-$(ARCH_SHORT).tar.gz | tar -xz
	@mv $(HOSTOS)-$(ARCH_SHORT)/helm $(HELM)
	@rm -rf helm.tgz $(HOSTOS)-$(ARCH_SHORT)

.PHONY: helm-lint
helm-lint: helm
#Verify lint works when region and apiToken are passed, and when it is passed as reference.
	@$(HELM) lint deploy/chart --set apiToken="apiToken",region="us-east"
	@$(HELM) lint deploy/chart --set secretRef.apiTokenRef="apiToken",secretRef.name="api",secretRef.regionRef="us-east"

.PHONY: helm-template
helm-template: helm
#Verify template works when region and apiToken are passed, and when it is passed as reference.
	@$(HELM) template foo deploy/chart --set apiToken="apiToken",region="us-east" > /dev/null
	@$(HELM) template foo deploy/chart --set secretRef.apiTokenRef="apiToken",secretRef.name="api",secretRef.regionRef="us-east" > /dev/null

.PHONY: kubectl
kubectl: $(KUBECTL) ## Download kubectl locally if necessary.
$(KUBECTL): $(LOCALBIN)
	curl -fsSL https://dl.k8s.io/release/$(KUBECTL_VERSION)/bin/$(OS)/$(ARCH_SHORT)/kubectl -o $(KUBECTL)
	chmod +x $(KUBECTL)


.PHONY: clusterctl
clusterctl: $(CLUSTERCTL) ## Download clusterctl locally if necessary.
$(CLUSTERCTL): $(LOCALBIN)
	curl -fsSL https://github.com/kubernetes-sigs/cluster-api/releases/download/$(CLUSTERCTL_VERSION)/clusterctl-$(OS)-$(ARCH_SHORT) -o $(CLUSTERCTL)
	chmod +x $(CLUSTERCTL)
